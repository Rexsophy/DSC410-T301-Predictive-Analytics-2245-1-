{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d1c7fc6a-9198-495d-b846-e255e1f88067",
      "metadata": {
        "id": "d1c7fc6a-9198-495d-b846-e255e1f88067"
      },
      "source": [
        "#### Rex Gayas\n",
        "#### Week 10 Exercise 10.2 Spring 2024\n",
        "#### DSC400-T301 Big Data, Technology, and Algo (2245-1)\n",
        "#### Classification in PySpark and Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 10**"
      ],
      "metadata": {
        "id": "YaI5o7bwb9Oq"
      },
      "id": "YaI5o7bwb9Oq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 10.1"
      ],
      "metadata": {
        "id": "AnqvdHNohhAu"
      },
      "id": "AnqvdHNohhAu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Initialize the SparkSession\n",
        "spark = SparkSession.builder.appName(\"DSC 400 Assignment 10\").getOrCreate()\n",
        "\n",
        "# Download the sample data file\n",
        "!wget https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt -P /content/data/mllib/\n",
        "\n",
        "# Path to the sample data\n",
        "sample_libsvm_data_path = \"/content/data/mllib/sample_libsvm_data.txt\"\n",
        "\n",
        "# Load training data\n",
        "training = spark.read.format(\"libsvm\").load(sample_libsvm_data_path)\n",
        "\n",
        "# Create LogisticRegression instance and set parameters\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "\n",
        "# Fit the model\n",
        "lrModel = lr.fit(training)\n",
        "\n",
        "# Print coefficients and intercept for Logistic Regression\n",
        "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
        "print(\"Intercept: \" + str(lrModel.intercept))\n",
        "\n",
        "# Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example\n",
        "trainingSummary = lrModel.summary\n",
        "\n",
        "# Obtain the objective per iteration\n",
        "objectiveHistory = trainingSummary.objectiveHistory\n",
        "print(\"objectiveHistory:\")\n",
        "for objective in objectiveHistory:\n",
        "    print(objective)\n",
        "\n",
        "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
        "trainingSummary.roc.show()\n",
        "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
        "\n",
        "# Set the model threshold to maximize F-Measure\n",
        "fMeasure = trainingSummary.fMeasureByThreshold\n",
        "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
        "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
        "    .select('threshold').head()['threshold']\n",
        "lrModel.setThreshold(bestThreshold)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MofD4_8-emkl",
        "outputId": "6c5adb86-f6ac-45fe-c059-25a71f0c179d"
      },
      "id": "MofD4_8-emkl",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-19 01:32:29--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 104736 (102K) [text/plain]\n",
            "Saving to: ‘/content/data/mllib/sample_libsvm_data.txt’\n",
            "\n",
            "\rsample_libsvm_data.   0%[                    ]       0  --.-KB/s               \rsample_libsvm_data. 100%[===================>] 102.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-19 01:32:29 (4.50 MB/s) - ‘/content/data/mllib/sample_libsvm_data.txt’ saved [104736/104736]\n",
            "\n",
            "Coefficients: (692,[272,300,323,350,351,378,379,405,406,407,428,433,434,435,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.520689871384125e-05,-8.115773146847006e-05,3.814692771846427e-05,0.0003776490540424338,0.0003405148366194403,0.0005514455157343107,0.0004085386116096912,0.0004197467332749452,0.0008119171358670031,0.000502770837266875,-2.3929260406600902e-05,0.0005745048020902297,0.0009037546426803677,7.818229700243899e-05,-2.1787551952911914e-05,-3.402165821789542e-05,0.0004966517360637633,0.0008190557828370372,-8.017982139522613e-05,-2.743169403783527e-05,0.0004810832226238988,0.0004840801762677878,-8.926472920009901e-06,-0.00034148812330427297,-8.950592574121382e-05,0.00048645469116892156,-8.478698005186097e-05,-0.00042347832158317705,-7.296535777631246e-05])\n",
            "Intercept: -0.5991460286401438\n",
            "objectiveHistory:\n",
            "0.6833149135741672\n",
            "0.6661906127558117\n",
            "0.6207433672479603\n",
            "0.6131541253123869\n",
            "0.6059149689952393\n",
            "0.5923656241678249\n",
            "0.589823308283802\n",
            "0.5868012627420282\n",
            "0.5844432058719142\n",
            "0.5830790068041746\n",
            "0.5807015754032354\n",
            "+---+--------------------+\n",
            "|FPR|                 TPR|\n",
            "+---+--------------------+\n",
            "|0.0|                 0.0|\n",
            "|0.0|0.017543859649122806|\n",
            "|0.0| 0.03508771929824561|\n",
            "|0.0| 0.05263157894736842|\n",
            "|0.0| 0.07017543859649122|\n",
            "|0.0| 0.08771929824561403|\n",
            "|0.0| 0.10526315789473684|\n",
            "|0.0| 0.12280701754385964|\n",
            "|0.0| 0.14035087719298245|\n",
            "|0.0| 0.15789473684210525|\n",
            "|0.0| 0.17543859649122806|\n",
            "|0.0| 0.19298245614035087|\n",
            "|0.0| 0.21052631578947367|\n",
            "|0.0| 0.22807017543859648|\n",
            "|0.0| 0.24561403508771928|\n",
            "|0.0|  0.2631578947368421|\n",
            "|0.0|  0.2807017543859649|\n",
            "|0.0|  0.2982456140350877|\n",
            "|0.0|  0.3157894736842105|\n",
            "|0.0|  0.3333333333333333|\n",
            "+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "areaUnderROC: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionModel: uid=LogisticRegression_0e3fee72adc0, numClasses=2, numFeatures=692"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialized a “SparkSession” and downloaded the “sample_libsvm_data.txt” dataset from a remote URL. The dataset was loaded into a Spark DataFrame using the “libsvm” format. Created a LogisticRegression” model with specific parameters (“maxIter=10”, “regParam=0.3”, “elasticNetParam=0.8”) and trained it on the dataset."
      ],
      "metadata": {
        "id": "Eqx2iEx_hVVy"
      },
      "id": "Eqx2iEx_hVVy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shows the coefficients of the model, the intercept, and the ROC curve data. The “objectiveHistory” array provides the values of the objective function at each iteration during model training, indicating the model's convergence. The ROC curve data displays the false positive rate (FPR) and true positive rate (TPR), and the area under the ROC (AUC) being 1.0 suggests a perfect fit for this dataset."
      ],
      "metadata": {
        "id": "3csAFRi-hV5e"
      },
      "id": "3csAFRi-hV5e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 10.2"
      ],
      "metadata": {
        "id": "jKhsQ2n1h1o5"
      },
      "id": "jKhsQ2n1h1o5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load the dataset\n",
        "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
        "dataframe = pd.read_csv(file_url)\n",
        "\n",
        "# Display the shape of the dataset and the first few rows\n",
        "print(dataframe.shape)\n",
        "print(dataframe.head())\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "val_dataframe = dataframe.sample(frac=0.2, random_state=1337)\n",
        "train_dataframe = dataframe.drop(val_dataframe.index)\n",
        "\n",
        "print(\n",
        "    f\"Using {len(train_dataframe)} samples for training \"\n",
        "    f\"and {len(val_dataframe)} for validation\"\n",
        ")\n",
        "\n",
        "# Convert dataframes to tf.data.Dataset objects\n",
        "def dataframe_to_dataset(dataframe):\n",
        "    dataframe = dataframe.copy()\n",
        "    labels = dataframe.pop(\"target\")\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    return ds\n",
        "\n",
        "train_ds = dataframe_to_dataset(train_dataframe)\n",
        "val_ds = dataframe_to_dataset(val_dataframe)\n",
        "\n",
        "# Batch the datasets\n",
        "train_ds = train_ds.batch(32)\n",
        "val_ds = val_ds.batch(32)\n",
        "\n",
        "# Define functions for feature encoding\n",
        "def encode_numerical_feature(feature, name, dataset):\n",
        "    normalizer = layers.Normalization()\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    normalizer.adapt(feature_ds)\n",
        "    encoded_feature = normalizer(feature)\n",
        "    return encoded_feature\n",
        "\n",
        "def encode_categorical_feature(feature, name, dataset, is_string):\n",
        "    lookup_class = layers.StringLookup if is_string else layers.IntegerLookup\n",
        "    lookup = lookup_class(output_mode=\"binary\")\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    lookup.adapt(feature_ds)\n",
        "    encoded_feature = lookup(feature)\n",
        "    return encoded_feature\n",
        "\n",
        "# Define inputs for the model\n",
        "sex = keras.Input(shape=(1,), name=\"sex\", dtype=\"int64\")\n",
        "cp = keras.Input(shape=(1,), name=\"cp\", dtype=\"int64\")\n",
        "fbs = keras.Input(shape=(1,), name=\"fbs\", dtype=\"int64\")\n",
        "restecg = keras.Input(shape=(1,), name=\"restecg\", dtype=\"int64\")\n",
        "exang = keras.Input(shape=(1,), name=\"exang\", dtype=\"int64\")\n",
        "ca = keras.Input(shape=(1,), name=\"ca\", dtype=\"int64\")\n",
        "thal = keras.Input(shape=(1,), name=\"thal\", dtype=\"string\")\n",
        "age = keras.Input(shape=(1,), name=\"age\")\n",
        "trestbps = keras.Input(shape=(1,), name=\"trestbps\")\n",
        "chol = keras.Input(shape=(1,), name=\"chol\")\n",
        "thalach = keras.Input(shape=(1,), name=\"thalach\")\n",
        "oldpeak = keras.Input(shape=(1,), name=\"oldpeak\")\n",
        "slope = keras.Input(shape=(1,), name=\"slope\")\n",
        "\n",
        "all_inputs = [\n",
        "    sex, cp, fbs, restecg, exang, ca, thal,\n",
        "    age, trestbps, chol, thalach, oldpeak, slope,\n",
        "]\n",
        "\n",
        "# Encode features\n",
        "sex_encoded = encode_categorical_feature(sex, \"sex\", train_ds, False)\n",
        "cp_encoded = encode_categorical_feature(cp, \"cp\", train_ds, False)\n",
        "fbs_encoded = encode_categorical_feature(fbs, \"fbs\", train_ds, False)\n",
        "restecg_encoded = encode_categorical_feature(restecg, \"restecg\", train_ds, False)\n",
        "exang_encoded = encode_categorical_feature(exang, \"exang\", train_ds, False)\n",
        "ca_encoded = encode_categorical_feature(ca, \"ca\", train_ds, False)\n",
        "thal_encoded = encode_categorical_feature(thal, \"thal\", train_ds, True)\n",
        "age_encoded = encode_numerical_feature(age, \"age\", train_ds)\n",
        "trestbps_encoded = encode_numerical_feature(trestbps, \"trestbps\", train_ds)\n",
        "chol_encoded = encode_numerical_feature(chol, \"chol\", train_ds)\n",
        "thalach_encoded = encode_numerical_feature(thalach, \"thalach\", train_ds)\n",
        "oldpeak_encoded = encode_numerical_feature(oldpeak, \"oldpeak\", train_ds)\n",
        "slope_encoded = encode_numerical_feature(slope, \"slope\", train_ds)\n",
        "\n",
        "# Combine encoded features\n",
        "all_features = layers.concatenate([\n",
        "    sex_encoded, cp_encoded, fbs_encoded, restecg_encoded,\n",
        "    exang_encoded, ca_encoded, thal_encoded,\n",
        "    age_encoded, trestbps_encoded, chol_encoded,\n",
        "    thalach_encoded, oldpeak_encoded, slope_encoded,\n",
        "])\n",
        "\n",
        "# Build the model\n",
        "x = layers.Dense(32, activation=\"relu\")(all_features)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(all_inputs, output)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Visualize the model\n",
        "keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_ds, epochs=50, validation_data=val_ds)\n",
        "\n",
        "# Inference on new data\n",
        "sample = {\n",
        "    \"age\": 60,\n",
        "    \"sex\": 1,\n",
        "    \"cp\": 1,\n",
        "    \"trestbps\": 145,\n",
        "    \"chol\": 233,\n",
        "    \"fbs\": 1,\n",
        "    \"restecg\": 2,\n",
        "    \"thalach\": 150,\n",
        "    \"exang\": 0,\n",
        "    \"oldpeak\": 2.3,\n",
        "    \"slope\": 3,\n",
        "    \"ca\": 0,\n",
        "    \"thal\": \"fixed\",\n",
        "}\n",
        "\n",
        "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
        "predictions = model.predict(input_dict)\n",
        "\n",
        "print(\n",
        "    f\"This particular patient had a {100 * predictions[0][0]:.1f} \"\n",
        "    \"percent probability of having a heart disease, \"\n",
        "    \"as evaluated by our model.\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHVRrsBogde3",
        "outputId": "1e4a1341-4956-4471-e07b-ea3545e8392b"
      },
      "id": "NHVRrsBogde3",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(303, 14)\n",
            "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
            "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
            "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
            "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
            "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
            "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
            "\n",
            "   ca        thal  target  \n",
            "0   0       fixed       0  \n",
            "1   3      normal       1  \n",
            "2   2  reversible       0  \n",
            "3   0      normal       0  \n",
            "4   0      normal       0  \n",
            "Using 242 samples for training and 61 for validation\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 2s 58ms/step - loss: 0.5716 - accuracy: 0.7107 - val_loss: 0.5284 - val_accuracy: 0.7377\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5169 - accuracy: 0.7851 - val_loss: 0.4925 - val_accuracy: 0.7541\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4824 - accuracy: 0.7727 - val_loss: 0.4661 - val_accuracy: 0.7705\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4620 - accuracy: 0.8140 - val_loss: 0.4462 - val_accuracy: 0.7705\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4653 - accuracy: 0.7851 - val_loss: 0.4317 - val_accuracy: 0.7705\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4403 - accuracy: 0.8017 - val_loss: 0.4209 - val_accuracy: 0.7705\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4092 - accuracy: 0.8306 - val_loss: 0.4127 - val_accuracy: 0.7705\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4165 - accuracy: 0.8058 - val_loss: 0.4054 - val_accuracy: 0.7705\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4194 - accuracy: 0.8099 - val_loss: 0.3991 - val_accuracy: 0.7705\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3883 - accuracy: 0.8430 - val_loss: 0.3944 - val_accuracy: 0.7705\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3808 - accuracy: 0.8471 - val_loss: 0.3915 - val_accuracy: 0.7869\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3617 - accuracy: 0.8430 - val_loss: 0.3893 - val_accuracy: 0.7869\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3641 - accuracy: 0.8512 - val_loss: 0.3871 - val_accuracy: 0.7869\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3743 - accuracy: 0.8430 - val_loss: 0.3858 - val_accuracy: 0.7869\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3701 - accuracy: 0.8264 - val_loss: 0.3851 - val_accuracy: 0.7869\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3579 - accuracy: 0.8264 - val_loss: 0.3844 - val_accuracy: 0.8033\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3370 - accuracy: 0.8471 - val_loss: 0.3834 - val_accuracy: 0.8033\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3419 - accuracy: 0.8264 - val_loss: 0.3824 - val_accuracy: 0.8033\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3237 - accuracy: 0.8678 - val_loss: 0.3821 - val_accuracy: 0.8033\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3502 - accuracy: 0.8223 - val_loss: 0.3823 - val_accuracy: 0.7869\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3080 - accuracy: 0.8471 - val_loss: 0.3817 - val_accuracy: 0.7869\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3509 - accuracy: 0.8306 - val_loss: 0.3810 - val_accuracy: 0.8033\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3301 - accuracy: 0.8595 - val_loss: 0.3804 - val_accuracy: 0.7869\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3359 - accuracy: 0.8306 - val_loss: 0.3808 - val_accuracy: 0.8033\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3161 - accuracy: 0.8636 - val_loss: 0.3811 - val_accuracy: 0.8033\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3162 - accuracy: 0.8678 - val_loss: 0.3811 - val_accuracy: 0.8033\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3130 - accuracy: 0.8471 - val_loss: 0.3820 - val_accuracy: 0.8033\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2902 - accuracy: 0.8719 - val_loss: 0.3820 - val_accuracy: 0.8033\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2935 - accuracy: 0.8595 - val_loss: 0.3824 - val_accuracy: 0.8033\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2724 - accuracy: 0.8884 - val_loss: 0.3831 - val_accuracy: 0.8033\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3012 - accuracy: 0.8802 - val_loss: 0.3838 - val_accuracy: 0.8033\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2986 - accuracy: 0.8843 - val_loss: 0.3850 - val_accuracy: 0.8033\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2857 - accuracy: 0.8719 - val_loss: 0.3856 - val_accuracy: 0.8033\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2829 - accuracy: 0.8760 - val_loss: 0.3862 - val_accuracy: 0.8033\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3043 - accuracy: 0.8595 - val_loss: 0.3866 - val_accuracy: 0.8033\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2857 - accuracy: 0.8884 - val_loss: 0.3874 - val_accuracy: 0.8033\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2825 - accuracy: 0.8843 - val_loss: 0.3867 - val_accuracy: 0.8033\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2735 - accuracy: 0.8760 - val_loss: 0.3866 - val_accuracy: 0.8033\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2673 - accuracy: 0.8760 - val_loss: 0.3865 - val_accuracy: 0.8033\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2753 - accuracy: 0.8802 - val_loss: 0.3873 - val_accuracy: 0.8033\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2779 - accuracy: 0.9008 - val_loss: 0.3884 - val_accuracy: 0.8033\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2951 - accuracy: 0.9008 - val_loss: 0.3882 - val_accuracy: 0.8033\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2641 - accuracy: 0.8967 - val_loss: 0.3883 - val_accuracy: 0.8033\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2488 - accuracy: 0.8884 - val_loss: 0.3894 - val_accuracy: 0.8033\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2580 - accuracy: 0.8719 - val_loss: 0.3906 - val_accuracy: 0.8033\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2516 - accuracy: 0.8802 - val_loss: 0.3912 - val_accuracy: 0.8033\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2671 - accuracy: 0.8843 - val_loss: 0.3917 - val_accuracy: 0.8033\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2577 - accuracy: 0.8926 - val_loss: 0.3916 - val_accuracy: 0.8033\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2645 - accuracy: 0.8967 - val_loss: 0.3930 - val_accuracy: 0.8033\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2581 - accuracy: 0.8802 - val_loss: 0.3937 - val_accuracy: 0.8033\n",
            "1/1 [==============================] - 0s 314ms/step\n",
            "This particular patient had a 13.9 percent probability of having a heart disease, as evaluated by our model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performed binary classification to predict heart disease using a neural network built with TensorFlow and Keras. Loaded the dataset from a CSV file into a Pandas DataFrame, which contains 303 samples and 14 columns, each representing various patient attributes and the target label indicating the presence of heart disease. The dataset was split into training (242 samples) and validation (61 samples) sets. Converted these sets into “tf.data.Dataset” objects, enabling efficient data handling and preprocessing. Each numerical feature was normalized, and categorical features were encoded using integer and string lookups. The neural network model was built with layers for input, dense connections, and dropout regularization. The model was compiled and trained for 50 epochs, achieving a training accuracy of up to 90.08% and a validation accuracy of 80.33%. The output shows the loss and accuracy metrics for each epoch, indicating the model's performance improvement over time. Finally, made predictions on new data, estimating a particular patient's probability of having heart disease to be 13.9%. This shows the model's capability to make informed predictions based on the provided attributes."
      ],
      "metadata": {
        "id": "BoWPdLs5hQuu"
      },
      "id": "BoWPdLs5hQuu"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}